from ast import mod
import asyncio
import json
from langchain_ollama.chat_models import ChatOllama
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_ollama import OllamaLLM
import sys
import os
from pathlib import Path
from langchain_mcp_tools import convert_mcp_to_langchain_tools

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from common.llm_factory import create_llm


@tool
def get_weather(city: str) -> str:
    """Get the current weather for a specific city.

    Use this tool when the user asks about weather conditions in any city.

    Args:
        city: The name of the city to get weather for (e.g., "Beijing", "San Francisco")

    Returns:
        A string describing the current weather in the specified city
    """
    print(f"[tool-call]get_weather: {city}")
    return f"It's always sunny in {city}!"


def loadMCPConfig():
    # ä»é¡¹ç›®æ ¹ç›®å½•ä¸‹åŠ è½½ mcp.json
    with open(Path(__file__).parent.parent / "mcp.json", "r", encoding="utf-8") as f:
        return json.load(f)


async def process_stream_response(agent, messages):
    """å¤„ç†æµå¼å“åº”å¹¶è¿”å›AIçš„å›å¤"""
    ai_response = ""
    tool_was_called = False

    async for chunk in agent.astream({"messages": messages}, stream_mode="updates"):
        for node_name, node_data in chunk.items():
            if "messages" in node_data:
                for message in node_data["messages"]:
                    # å¤„ç†å¯¹è¯å†…å®¹
                    isToolMessage = hasattr(message, "name") and message.name
                    if hasattr(message, "content") and message.content:
                        # è¿‡æ»¤æ‰<think>æ ‡ç­¾å†…å®¹
                        content = message.content
                        if "<think>" in content:
                            # æå– <think> å’Œ </think> ä¹‹é—´çš„å†…å®¹
                            thinking = (
                                content.split("<think>")[-1]
                                .split("</think>")[0]
                                .strip()
                            )
                            if thinking:
                                print(f"ğŸ¤” æ€è€ƒä¸­: {thinking}")
                            # æå–</think>ä¹‹åçš„å†…å®¹
                            content = content.split("</think>")[-1].strip()
                        if content and not isToolMessage:
                            print(f"ğŸ’¬ [{node_name}]: {content}")
                            ai_response += content

                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if hasattr(message, "tool_calls") and message.tool_calls:
                        tool_was_called = True
                        for tool_call in message.tool_calls:
                            print(
                                f"ğŸ”§ å·¥å…·è°ƒç”¨: {tool_call['name']}({tool_call['args']})"
                            )

                    # å¤„ç†å·¥å…·ç»“æœ
                    if isToolMessage:
                        tool_was_called = True
                        print(f"âœ… å·¥å…·ç»“æœ [{message.name}]: {message.content}")

    return ai_response


async def main():
    """ä¸»å¯¹è¯å¾ªç¯"""
    print("ğŸ¤– æ¬¢è¿ä½¿ç”¨ LangGraph CLI å¯¹è¯ç³»ç»Ÿ!")
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯")
    print("=" * 50)

    # åˆ›å»º agent
    llm = create_llm()
    tools, cleanup = await convert_mcp_to_langchain_tools(loadMCPConfig()["mcpServers"])

    print("mcp tools: ", tools)

    agent = create_react_agent(
        model=llm,
        tools=[get_weather, *tools],
        prompt="""You are a helpful assistant with access to tools. """,
    )

    # åˆå§‹åŒ–å¯¹è¯å†å²
    messages = []

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()

            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["quit", "exit", "é€€å‡º"]:
                print("ğŸ‘‹ å†è§!")
                break

            # è·³è¿‡ç©ºè¾“å…¥
            if not user_input:
                continue

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            messages.append(HumanMessage(content=user_input))

            print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

            # å¤„ç†AIå“åº”
            ai_response = await process_stream_response(agent, messages)

            # æ·»åŠ AIæ¶ˆæ¯åˆ°å†å²
            if ai_response:
                messages.append(AIMessage(content=ai_response))

            print()  # æ¢è¡Œ

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            continue
    await cleanup()


if __name__ == "__main__":
    asyncio.run(main())
